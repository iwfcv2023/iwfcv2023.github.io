---
title: Program
layout: default
permalink: /program
class: home
---

<div style="height: 1rem;"></div>
<div class="hr"></div>
<div style="height: 1rem;"></div>

## Time Table

<div style="font-weight: bold; color: gray; text-align: right;">
    Program : 
    <a href="/assets/Program_paper_v4.pdf" download="Program.pdf">Download Link</a>
</div>
<div style="font-weight: bold; color: gray; text-align: right;">
    TimeTable : 
    <a href="/assets/Time_Table_v3.pdf" download="TimeTable.pdf">Download Link</a>
</div>

<img style="border-radius: 0em; margin: 0 20% 0 20%; max-width: 60%; height: auto; display: block; box-shadow: 0px 0px 0px 0px rgba(0, 0, 0, .5);" src="/assets/TimeTable.png" alt="TimeTable">

<div style="height: 1rem;"></div>
<div class="hr"></div>
<div style="height: 1rem;"></div>

## Keynote Speaker

<div style="height: 1rem;"></div>
<div class="hr"></div>
<div style="height: 1rem;"></div>
<div class="col" style="display: flex; flex-direction: column;">
    <p style="font-size: 1.5em; font-weight: bold; margin-bottom: 50px; color:rgb(65, 0, 249); text-align: center;">
        Keynote Speaker #1
    </p>
    <div class="row">
        <div style="width:25%; float: left; position: relative; min-height: 1px; padding-right: 15px; padding-left:15px">
            <img style="margin: 0px" src="https://manuscriptlink-society-file.s3.ap-northeast-1.amazonaws.com/kism/conference/sma2022fall/keynote/1.jpg" alt="Dr. Hirokatsu Kataoka, AIST, Japan">
            <p style="color: black; font-weight: bold; font-size: 1.2em; margin-top: 10px; padding: 0 10px;">
                    Dr. In So Kweon <br> KAIST, Korea
            </p>
        </div>
        <div style="width:65%; float: right; position: relative; min-height: 1px; padding-right: 15px; padding-left:15px">
            <p style="font-size: 1.2em; font-weight: bold; color: black; text-align: justify;">
                Title: Deep Attention Models for Object Recognition
            </p>
            <p style="text-align: justify;">
                <sapn style="color: black; font-weight: bold;">Abstract : </sapn> In recent years, the performance of Deep Learning based approaches for object recognition has improved dramatically and even surpassed human performance in some benchmarking datasets. Specifically, Deep attention models have been very effective to improve recognition performance. In this talk, we present two convolutional attention models inspired by human visual systems, in which the object is defined by the presence or absence of “Local Visual Properties” and by “object parts” with their “Contextual Relations”. We also present a simple and effective video mask transformer model that is widely applicable to multiple video segmentation tasks. TubeFormer-DeepLab directly predicts video tubes with task-specific labels (either pure semantic categories, or both semantic categories and instance identities), which not only significantly simplifies video segmentation models, but also advances state-of-the-art results on multiple video segmentation benchmarks.
            </p>
            <p style="text-align: justify;">
                <sapn style="color: black; font-weight: bold;">Biography : </sapn> Professor In So Kweon received the B.S. and the M.S. degrees in Mechanical Design and Production Engineering from Seoul National University, Korea, in 1981 and 1983, respectively, and the Ph.D. degree in Robotics from the Robotics Institute at Carnegie Mellon University in 1990. He worked for Toshiba R&D Center and joined KAIST in 1992. He is a KEPCO Chair professor of the School of Electrical Engineering and had been the director for the National Core Research Center – P3 DigiCar Center at KAIST (2010~2017). His research focuses on Computer Vision and Robotics. He has published 3 research books, and more than 500 papers in leading journals and conference proceedings, including 100+ in prestigious CVPR, ICCV, and ECCV. He is also active in professional service. Currently, he is the President of the Asia Federation of Computer Vision (AFCV). He served on the Editorial Board of the International Journal of Computer Vision for ten years since 2005. He has also organized 5 international conferences either as a general chair or a program chair, including IEEE-CVF ICCV 2019. He was awarded “2016 Faculty Research Excellence Award”, “2020 Grand Prize for Academic Excellence”, and “2021 Hyung-Gyu Im LINKGENESIS Best Teacher Award” by KAIST and conferred a Prime Minister Award by the Korean Government for his contribution to DRC-HUBO+ to win the DARPA Robotics Challenge in 2015. He also received several awards from international conferences, including "The Best Paper Award of the IEEE Transaction on CSVT 2014" and "The Best Student Paper Runnerup Award in the IEEE-CVPR 2009".
            </p>
        </div>
    </div>
    <div style="height: 1rem;"></div>
    <div class="hr"></div>
    <div style="height: 1rem;"></div>
    <p style="font-size: 1.5em; font-weight: bold; margin-bottom: 50px; color:rgb(65, 0, 249); text-align: center;">
        Keynote Speaker #2
    </p>
    <div class="row" style="margin:10px;">
        <div style="width:25%; float: left; position: relative; min-height: 1px; padding-right: 15px; padding-left:15px">
            <img style="margin: 0px" src="https://hirokatsukataoka.net/image/hirokatsukataoka_2210.png" alt="Dr. Hirokatsu Kataoka, AIST, Japan">
            <p style="color: black; font-weight: bold; font-size: 1.2em; margin-top: 10px; padding: 0 10px;">
                    Dr. Hirokatsu Kataoka <br>AIST, Japan
            </p>
        </div>
        <div style="width:65%; float: right; position: relative; min-height: 1px; padding-right: 15px; padding-left:15px">
            <p style="font-size: 1.2em; font-weight: bold; color: black; text-align: justify;">
                Title: Pre-training without Natural Images
            </p>
            <p style="text-align: justify;">
                <sapn style="color: black; font-weight: bold;">Abstract : </sapn> The talk introduces a novel concept called Formula-driven Supervised Learning (FDSL) for using convolutional neural networks and vision transformers pre-trained without real images for real image recognition. Instead of real images, the pre-training phase employs fractals/contours generated from mathematical formulas as image patterns and their category labels, allowing for an infinitely large labeled image dataset. The proposed FDSL framework differs from other learning strategy like self-supervised learning, and doesn't require defining object categories and preparing real images. The experimental results show that FDSL pre-training outperforms some pre-trained models and captures unique features in a model visualization.
            </p>
            <p style="text-align: justify;">
                <sapn style="color: black; font-weight: bold;">Biography : </sapn> Hirokatsu Kataoka received his Ph.D. in engineering from Keio University in 2014. His research experience includes visiting scientist at Technical University of Munich (TUM) and JSPS Fellow (PD) at the University of Tokyo. Currently, he is a Chief Senior Researcher at National Institute of Advanced Industrial Science and Technology (AIST). He also leads the cvpaper.challenge which conducts comprehensive survey and collaborative research in the field of computer vision and related academic fields. His research interest includes computer vision and pattern recognition, especially in large-scale dataset for image and video recognition. He has received ACCV 2020 Best Paper Honorable Mention Award, AIST 2019 Best Paper Award, and ECCV 2016 Workshop Brave New Idea.
            </p>
        </div>
    </div>
</div>

<br>